{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, TrainerCallback\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape : {'test': (4358, 1), 'train': (36718, 1), 'validation': (3760, 1)}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "print(f\"dataset shape : {dataset.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reham\\OneDrive\\Desktop\\huawei_task\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load pre-trained GPT-2 model and tokenizer \n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Tokenizer padding\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation before fine-tuning\n",
    "def generate_text_before(prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=max_length, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before fine-tuning:\n",
      "A PhD student is excited to join Huawei research team and work with them on a new project.\n",
      "\n",
      "\"We are excited to be working with Huawei on a new project that will allow us to develop a new wireless technology that will enable us to deliver\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A PhD student is excited to join Huawei research team\"\n",
    "print(\"Before fine-tuning:\")\n",
    "print(generate_text_before(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Tokenize the text\n",
    "def tokenize_function(examples):\n",
    "    #1024 is set as the max_length to utilizes the full context window of GPT-2,\n",
    "    #  which is better for understanding long sequences of text, however it uses much memory comparing to 512\n",
    "    tokenized = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    #Use input_ids as labels for training\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For effeciency and simplicity the text columns is removed\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback to log losses\n",
    "class LossLogger(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.eval_losses = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            # Log training loss\n",
    "            if 'loss' in logs:\n",
    "                self.train_losses.append(logs['loss'])\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics is not None:\n",
    "            # Log evaluation loss\n",
    "            if 'eval_loss' in metrics:\n",
    "                self.eval_losses.append(metrics['eval_loss'])\n",
    "\n",
    "# Initialize the loss logger\n",
    "loss_logger = LossLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reham\\OneDrive\\Desktop\\huawei_task\\venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Training arguments for fine-tuning\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,#7\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    eval_steps=50,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),  # Enables mixed precision if GPU supports it\n",
    "    save_steps=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reham\\OneDrive\\Desktop\\huawei_task\\venv\\Lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Allocated: {torch.cuda.memory_allocated() / (1024 ** 2):.2f} MB\")\n",
    "        print(f\"Cached: {torch.cuda.memory_reserved() / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 487.47 MB\n",
      "Cached: 542.00 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c843d356e244469a45bfc016431a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reham\\OneDrive\\Desktop\\huawei_task\\venv\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0776, 'grad_norm': 1.0430114269256592, 'learning_rate': 4.9923747276688455e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6f6bc6a60a4f6ba1b2c9cd999723f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4658660590648651, 'eval_runtime': 294.258, 'eval_samples_per_second': 12.778, 'eval_steps_per_second': 3.194, 'epoch': 0.01}\n",
      "{'loss': 0.3767, 'grad_norm': 1.5032342672348022, 'learning_rate': 4.983297022512709e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f697b2e76eb7481aa45ea488a382959c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4543038606643677, 'eval_runtime': 295.1228, 'eval_samples_per_second': 12.74, 'eval_steps_per_second': 3.185, 'epoch': 0.01}\n",
      "{'loss': 0.4624, 'grad_norm': 1.493654489517212, 'learning_rate': 4.9742193173565725e-05, 'epoch': 0.02}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be1b9a659b546cabcd0b0cda9ff4f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4504323899745941, 'eval_runtime': 292.9769, 'eval_samples_per_second': 12.834, 'eval_steps_per_second': 3.208, 'epoch': 0.02}\n",
      "{'loss': 0.4658, 'grad_norm': 1.2388556003570557, 'learning_rate': 4.9651416122004356e-05, 'epoch': 0.02}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f8c25abc44428691009ec365429735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44561636447906494, 'eval_runtime': 294.1831, 'eval_samples_per_second': 12.781, 'eval_steps_per_second': 3.195, 'epoch': 0.02}\n",
      "{'loss': 0.4, 'grad_norm': 1.8811702728271484, 'learning_rate': 4.9560639070442995e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ce7875001048669d66051003a022ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44519656896591187, 'eval_runtime': 293.9984, 'eval_samples_per_second': 12.789, 'eval_steps_per_second': 3.197, 'epoch': 0.03}\n",
      "{'loss': 0.4432, 'grad_norm': 0.1982816904783249, 'learning_rate': 4.946986201888163e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe651bade5343be98a15bab9ed73240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4427814185619354, 'eval_runtime': 294.0213, 'eval_samples_per_second': 12.788, 'eval_steps_per_second': 3.197, 'epoch': 0.03}\n",
      "{'loss': 0.4133, 'grad_norm': 0.31772828102111816, 'learning_rate': 4.9379084967320265e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2c62036cba4d2dbdfc40e9d147fb59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44387614727020264, 'eval_runtime': 293.9832, 'eval_samples_per_second': 12.79, 'eval_steps_per_second': 3.197, 'epoch': 0.04}\n",
      "{'loss': 0.4972, 'grad_norm': 0.20903582870960236, 'learning_rate': 4.9288307915758896e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0620c991f54be3a76710d2b42af56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4424620270729065, 'eval_runtime': 293.9568, 'eval_samples_per_second': 12.791, 'eval_steps_per_second': 3.198, 'epoch': 0.04}\n",
      "{'loss': 0.4466, 'grad_norm': 1.5317144393920898, 'learning_rate': 4.9197530864197535e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25025637cd7842738795f5110fd5611c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44207483530044556, 'eval_runtime': 293.8847, 'eval_samples_per_second': 12.794, 'eval_steps_per_second': 3.199, 'epoch': 0.05}\n",
      "{'loss': 0.4653, 'grad_norm': 1.5647974014282227, 'learning_rate': 4.9106753812636166e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20af0cda5e264eb99815ce0710a3c3f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44046053290367126, 'eval_runtime': 294.6865, 'eval_samples_per_second': 12.759, 'eval_steps_per_second': 3.19, 'epoch': 0.05}\n",
      "{'loss': 0.5072, 'grad_norm': 0.8568047285079956, 'learning_rate': 4.90159767610748e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00526de297d1479ea4324220715aa968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43968459963798523, 'eval_runtime': 293.8319, 'eval_samples_per_second': 12.796, 'eval_steps_per_second': 3.199, 'epoch': 0.06}\n",
      "{'loss': 0.449, 'grad_norm': 0.6725171804428101, 'learning_rate': 4.8925199709513436e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413d23a114e84b36a22d0d442c5ce3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44085580110549927, 'eval_runtime': 292.2246, 'eval_samples_per_second': 12.867, 'eval_steps_per_second': 3.217, 'epoch': 0.07}\n",
      "{'loss': 0.5358, 'grad_norm': 0.741547703742981, 'learning_rate': 4.8834422657952074e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79dba1b645e4daf9ddbe5a953505043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43882766366004944, 'eval_runtime': 294.6885, 'eval_samples_per_second': 12.759, 'eval_steps_per_second': 3.19, 'epoch': 0.07}\n",
      "{'loss': 0.3852, 'grad_norm': 1.320408821105957, 'learning_rate': 4.8743645606390706e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abece91c5da4159aed728dd0142546a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4392436444759369, 'eval_runtime': 293.9373, 'eval_samples_per_second': 12.792, 'eval_steps_per_second': 3.198, 'epoch': 0.08}\n",
      "{'loss': 0.4922, 'grad_norm': 1.5941028594970703, 'learning_rate': 4.865286855482934e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9c11aa2cde4e1aa3fed1c71d771120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4377095103263855, 'eval_runtime': 292.2422, 'eval_samples_per_second': 12.866, 'eval_steps_per_second': 3.217, 'epoch': 0.08}\n",
      "{'loss': 0.464, 'grad_norm': 1.3240922689437866, 'learning_rate': 4.8562091503267976e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e650519c92c45f8a77b77cb94e1c9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43750420212745667, 'eval_runtime': 293.5146, 'eval_samples_per_second': 12.81, 'eval_steps_per_second': 3.203, 'epoch': 0.09}\n",
      "{'loss': 0.4144, 'grad_norm': 1.5798345804214478, 'learning_rate': 4.8471314451706614e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ac37abbe064fe4a1a4ec6821e26d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43705129623413086, 'eval_runtime': 295.9506, 'eval_samples_per_second': 12.705, 'eval_steps_per_second': 3.176, 'epoch': 0.09}\n",
      "{'loss': 0.4295, 'grad_norm': 1.4635580778121948, 'learning_rate': 4.8380537400145245e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b157a89f14bd4d259fb8db7af3293741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43603038787841797, 'eval_runtime': 296.8854, 'eval_samples_per_second': 12.665, 'eval_steps_per_second': 3.166, 'epoch': 0.1}\n",
      "{'loss': 0.3733, 'grad_norm': 0.6076154708862305, 'learning_rate': 4.828976034858388e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f08881b0a1a470ba87c7a3e3d0c6efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43775519728660583, 'eval_runtime': 296.7946, 'eval_samples_per_second': 12.669, 'eval_steps_per_second': 3.167, 'epoch': 0.1}\n",
      "{'loss': 0.4738, 'grad_norm': 0.19454355537891388, 'learning_rate': 4.8198983297022515e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a91b4c32b2142f88ba5d91156f5f0ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43598562479019165, 'eval_runtime': 294.8921, 'eval_samples_per_second': 12.75, 'eval_steps_per_second': 3.188, 'epoch': 0.11}\n",
      "{'loss': 0.4164, 'grad_norm': 1.638607382774353, 'learning_rate': 4.8108206245461154e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05506bb38bfd4998931d2d7624d3a549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4365951716899872, 'eval_runtime': 293.9611, 'eval_samples_per_second': 12.791, 'eval_steps_per_second': 3.198, 'epoch': 0.11}\n",
      "{'loss': 0.4652, 'grad_norm': 0.1091577410697937, 'learning_rate': 4.8017429193899785e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7300044d41414ba0b6e2156bd26475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4363793134689331, 'eval_runtime': 293.6937, 'eval_samples_per_second': 12.802, 'eval_steps_per_second': 3.201, 'epoch': 0.12}\n",
      "{'loss': 0.5012, 'grad_norm': 1.4645557403564453, 'learning_rate': 4.792665214233842e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a078e3ebbd64bbda5c8e4ce605ab1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4357193410396576, 'eval_runtime': 295.3178, 'eval_samples_per_second': 12.732, 'eval_steps_per_second': 3.183, 'epoch': 0.13}\n",
      "{'loss': 0.4357, 'grad_norm': 0.6720056533813477, 'learning_rate': 4.7835875090777055e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed08106544454668ab8857138404c28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4347008168697357, 'eval_runtime': 295.4059, 'eval_samples_per_second': 12.728, 'eval_steps_per_second': 3.182, 'epoch': 0.13}\n",
      "{'loss': 0.4622, 'grad_norm': 0.8318189382553101, 'learning_rate': 4.774509803921569e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ca66b75f274eb18dc356ab068adcd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.434119313955307, 'eval_runtime': 295.1456, 'eval_samples_per_second': 12.739, 'eval_steps_per_second': 3.185, 'epoch': 0.14}\n",
      "{'loss': 0.4996, 'grad_norm': 1.236153483390808, 'learning_rate': 4.7654320987654325e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262068528fd64457a2b3f1ae0639c31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4338391125202179, 'eval_runtime': 294.7708, 'eval_samples_per_second': 12.756, 'eval_steps_per_second': 3.189, 'epoch': 0.14}\n",
      "{'loss': 0.5093, 'grad_norm': 1.193713903427124, 'learning_rate': 4.7563543936092956e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc9f129778140bc806af72745787082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4341387152671814, 'eval_runtime': 294.9721, 'eval_samples_per_second': 12.747, 'eval_steps_per_second': 3.187, 'epoch': 0.15}\n",
      "{'loss': 0.581, 'grad_norm': 1.6505205631256104, 'learning_rate': 4.7472766884531595e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8995ba3f5b647c8b9c6448e99d8478d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4345899224281311, 'eval_runtime': 295.1211, 'eval_samples_per_second': 12.741, 'eval_steps_per_second': 3.185, 'epoch': 0.15}\n",
      "{'loss': 0.4391, 'grad_norm': 1.704192042350769, 'learning_rate': 4.7381989832970226e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb67a092ccc4101bb6d93ef7860ed8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43403932452201843, 'eval_runtime': 295.6185, 'eval_samples_per_second': 12.719, 'eval_steps_per_second': 3.18, 'epoch': 0.16}\n",
      "{'loss': 0.398, 'grad_norm': 0.246282696723938, 'learning_rate': 4.729121278140886e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c3f96e1014432e9b6bec447990b973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.433960884809494, 'eval_runtime': 295.1897, 'eval_samples_per_second': 12.738, 'eval_steps_per_second': 3.184, 'epoch': 0.16}\n",
      "{'loss': 0.4218, 'grad_norm': 1.2182611227035522, 'learning_rate': 4.7200435729847496e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19150b44be584fcfb00183e964cf2de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43404486775398254, 'eval_runtime': 295.3823, 'eval_samples_per_second': 12.729, 'eval_steps_per_second': 3.182, 'epoch': 0.17}\n",
      "{'loss': 0.4649, 'grad_norm': 0.7265605330467224, 'learning_rate': 4.7109658678286135e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ccfd9abcdc4f8d87db92773e1cca9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4332168400287628, 'eval_runtime': 289.9793, 'eval_samples_per_second': 12.966, 'eval_steps_per_second': 3.242, 'epoch': 0.17}\n",
      "{'loss': 0.4316, 'grad_norm': 0.39429306983947754, 'learning_rate': 4.7018881626724766e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fcc63d4b40b41d7ba771272129782c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43327122926712036, 'eval_runtime': 290.271, 'eval_samples_per_second': 12.953, 'eval_steps_per_second': 3.238, 'epoch': 0.18}\n",
      "{'loss': 0.4207, 'grad_norm': 0.20005400478839874, 'learning_rate': 4.69281045751634e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e259cf1acb154309b40abd187cefbf6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4344761073589325, 'eval_runtime': 290.26, 'eval_samples_per_second': 12.954, 'eval_steps_per_second': 3.238, 'epoch': 0.19}\n",
      "{'loss': 0.4201, 'grad_norm': 0.47727611660957336, 'learning_rate': 4.6837327523602036e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172434ba760f42899cc7d5fb881ff8b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43385419249534607, 'eval_runtime': 290.3171, 'eval_samples_per_second': 12.951, 'eval_steps_per_second': 3.238, 'epoch': 0.19}\n",
      "{'loss': 0.4768, 'grad_norm': 3.1601099967956543, 'learning_rate': 4.674655047204067e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e5f921e2304907a59462d409de336e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.434255450963974, 'eval_runtime': 291.8445, 'eval_samples_per_second': 12.884, 'eval_steps_per_second': 3.221, 'epoch': 0.2}\n",
      "{'loss': 0.4856, 'grad_norm': 3.4885709285736084, 'learning_rate': 4.6655773420479306e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec1e1ebf0394469bce65a145f9dc320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43378737568855286, 'eval_runtime': 296.3672, 'eval_samples_per_second': 12.687, 'eval_steps_per_second': 3.172, 'epoch': 0.2}\n",
      "{'loss': 0.3944, 'grad_norm': 0.18796782195568085, 'learning_rate': 4.656499636891794e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3bd3291a22496bb659b90ba9cf7e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4336297810077667, 'eval_runtime': 291.2641, 'eval_samples_per_second': 12.909, 'eval_steps_per_second': 3.227, 'epoch': 0.21}\n",
      "{'loss': 0.369, 'grad_norm': 1.1330397129058838, 'learning_rate': 4.6474219317356576e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866ff574ab984cfe9b7a5ad386a15944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4336042106151581, 'eval_runtime': 290.6313, 'eval_samples_per_second': 12.937, 'eval_steps_per_second': 3.234, 'epoch': 0.21}\n",
      "{'loss': 0.4048, 'grad_norm': 0.5846811532974243, 'learning_rate': 4.638344226579521e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b90110853e846829207727473f01bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43322551250457764, 'eval_runtime': 294.075, 'eval_samples_per_second': 12.786, 'eval_steps_per_second': 3.196, 'epoch': 0.22}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m      5\u001b[0m print_gpu_memory_usage()\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m print_gpu_memory_usage()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# End timing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\reham\\OneDrive\\Desktop\\huawei_task\\venv\\Lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\reham\\OneDrive\\Desktop\\huawei_task\\venv\\Lib\\site-packages\\transformers\\trainer.py:2393\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2394\u001b[0m ):\n\u001b[0;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 7: Fine-tune the model\n",
    "\n",
    "# Start timing\n",
    "start_time = time.perf_counter()\n",
    "print_gpu_memory_usage()\n",
    "trainer.train()\n",
    "print_gpu_memory_usage()\n",
    "# End timing\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training Time: {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access logged losses\n",
    "train_losses = loss_logger.train_losses\n",
    "eval_losses = loss_logger.eval_losses\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(range(len(eval_losses)), eval_losses, label='Evaluation Loss', color='orange', marker='o')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Evaluation Losses')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./gpt2-finetuned\")\n",
    "tokenizer.save_pretrained(\"./gpt2-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned GPT-2 model and tokenizer\n",
    "fine_tuned_model_path = \"./gpt2-finetuned\"  # Path to your fine-tuned model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation after fine-tuning\n",
    "def generate_text_after(prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=max_length, num_return_sequences=1, attention_mask=inputs[\"attention_mask\"], eos_token_id=None)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After fine-tuning:\")\n",
    "prompt = \"A PhD student is excited to join Huawei research team\"\n",
    "print(generate_text_after(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
